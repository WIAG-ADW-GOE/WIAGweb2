{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 6. wiag_to_factgrid.ipynb\n",
    "\n",
    "\n",
    "\n",
    " ## **Office Creation Notebook**\n",
    "\n",
    " This notebook takes data from wiag as the primary source, then joins it with\n",
    "\n",
    " * institution data from factgrid\n",
    "\n",
    " * diocese data from factgrid\n",
    "\n",
    " * role data from wiag\n",
    "\n",
    " * role data from factgrid\n",
    "\n",
    "\n",
    "\n",
    " and then creates a final quickstatements csv at the end to be uploaded to FactGrid.\n",
    "\n",
    "\n",
    "\n",
    " At every join operation, there is the possibility that some data in wiag has no corresponding data in factgrid.\n",
    "\n",
    " The notebook create a quickstatements csv to create the missing data whenever this happens.\n",
    "\n",
    " After creating the csv, it removes all the missing entries and moves on the next step (these cells are marked with two stars **). There are two possible routes to execute this notebook:\n",
    "\n",
    " 1. [Import the csv files to factgrid](https://database.factgrid.de/quickstatements/#/batch) whenever one is generated, and then run the notebook from the beginning up to that point.\n",
    "\n",
    " 2. Do not create any factgrid entries except at the very last step. This flow works since all missing entries are ignored after their corresponding csv file is generated.\n",
    "\n",
    "\n",
    "\n",
    " This is explained with the diagram below. The description of the shapes is below:\n",
    "\n",
    " * Diamonds: The diamonds indicate a join operation. After this operation you have entries that have been successfully added information to.\n",
    "\n",
    " * Circles: The circles indicate the records that were successfully joined. This means that there was more information added to the orignal record.\n",
    "\n",
    " * Rectangle: The rectanges indicate the records that failed the join operation.\n",
    "\n",
    "\n",
    "\n",
    " ![office_creation_flow.drawio.png](docs/images/office_creation_flow.drawio.png)\n",
    "\n",
    "\n",
    "\n",
    " The large arrow on the right that goes back up indicates that after a join operation is failed, you could use the generated files to fix the problems with the records and start from the top again.\n",
    "\n",
    "\n",
    "\n",
    " So the first route to execute this notebook is to follow the square blocks as soon as you encounter one and then take the arrow back to the start. The second route is to follow the circle blocks and go down until the final csv file is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import traceback\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r\"C:\\Users\\Public\\WIAGweb2\\notebooks\\sync_notebooks\\input_files\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"C:\\Users\\Public\\WIAGweb2\\notebooks\\sync_notebooks\\output_files\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_string = datetime.now().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Download data from WIAG\n",
    "\n",
    " ### Export data from WIAG database\n",
    "\n",
    "\n",
    "\n",
    " For this step you need to manually export the dataset by opening [phpMyAdmin](https://vwebfile.gwdg.de/phpmyadmin/) and then:\n",
    "\n",
    " 1. choose the wiagvokabulare database\n",
    "\n",
    " 2. run the saved 'Step 6 of the sync notebooks for WIAG' sql query\n",
    "\n",
    " 3. export the result to a csv file\n",
    "\n",
    "\n",
    "\n",
    " A detailed description can be found here: [Run_SQL_Query_and_Export_CSV.md](https://github.com/WIAG-ADW-GOE/WIAGweb2/blob/main/notebooks/sync_notebooks/docs/Run_SQL_Query_and_Export_CSV.md) (As a backup, [here is the saved query](https://github.com/WIAG-ADW-GOE/WIAGweb2/blob/main/notebooks/sync_notebooks/scripts/get_wiag_roles.sql))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " ### Import the files\n",
    "\n",
    " Please move the downloaded file to the `input_path` directory defined above or change the `input_path` to where the file is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'role.csv'\n",
    "input_path_file = os.path.join(input_path, input_file)\n",
    "wiag_roles_df = pl.read_csv(input_path_file, null_values='NULL', columns = [0, 2, 17], new_columns=['id', 'name', 'role_fg_id'])\n",
    "len(wiag_roles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Download data from WIAG\n",
    "\n",
    " https://wiag-vokabulare.uni-goettingen.de/query/can\n",
    "\n",
    "\n",
    "\n",
    " It's recommended to limit the export to one Domstift by first searching for that Domstift before exporting the 'CSV Amtsdaten' to make sure that the amount of roles to be added is manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domstift = \"Mainz\"\n",
    "# domstift = \"\" # in case you did not filter by Domstift, use this line instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'WIAG-Domherren-DB-Ämter' + domstift + '.csv'\n",
    "input_path_file = os.path.join(input_path, input_file)\n",
    "role_all_df = pl.read_csv(input_path_file, separator=';', infer_schema_length = None)\n",
    "len(role_all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_modified = datetime.fromtimestamp(os.path.getmtime(input_path_file))\n",
    "now = datetime.now()\n",
    "assert last_modified.day == now.day and last_modified.month == now.month, f'The file was last updated on {last_modified.strftime('%d.%m')}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Troubleshooting: Old file used\n",
    "\n",
    " You get an error when you run the line above if the file was not updated today.\n",
    "\n",
    " Suggested solutions:\n",
    "\n",
    " * update the file again by downloading it again\n",
    "\n",
    " * if you downloaded the data today, check the file name in input_file. It's pointing to a file that has old data.\n",
    "\n",
    " * (not recommended) continue if you are sure that you need to use old data. This is something that the developer might want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Download data from factgrid\n",
    "\n",
    "\n",
    "\n",
    " Troubleshooting: If any of the following requests to factgrid fail, try rerunning the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following cell looks up institutions with an entry in the 'Klosterdatenbank' and gets their id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://database.factgrid.de/sparql'\n",
    "query = (\n",
    "    \"\"\"SELECT ?item ?gsn WHERE {\n",
    "  ?item wdt:P471 ?gsn\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "# SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "\n",
    "# make request: \n",
    "r = requests.get(url, params={'query': query}, headers={\"Accept\": \"application/json\"})\n",
    "data = r.json()\n",
    "factgrid_institution_df = pl.json_normalize(data['results']['bindings'])\n",
    "factgrid_institution_df = factgrid_institution_df.cast({'gsn.value':pl.UInt32})\n",
    "\n",
    "len(factgrid_institution_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following cell looks up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://database.factgrid.de/sparql'\n",
    "query = (\n",
    "\"\"\"\n",
    "SELECT DISTINCT ?item ?wiagid ?label ?alternative WHERE {\n",
    "  ?item wdt:P2/wdt:P3* wd:Q164535.\n",
    "  #?item schema:description ?itemDesc.\n",
    "  ?item rdfs:label ?label.\n",
    "  OPTIONAL {?item schema:description ?itemDesc.}\n",
    "  OPTIONAL {?item skos:altLabel ?alternative. }\n",
    "  OPTIONAL {?item wdt:P601 ?wiagid.}\n",
    "  FILTER(LANG(?label) in (\"en\", \"de\"))\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# make request: \n",
    "r = requests.get(url, params={'query': query}, headers={\"Accept\": \"application/json\"})\n",
    "data = r.json()\n",
    "factgrid_diocese_df = pl.json_normalize(data['results']['bindings'])\n",
    "\n",
    "len(factgrid_diocese_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://database.factgrid.de/sparql'\n",
    "query = (\n",
    "\"\"\"\n",
    "SELECT ?item ?label WHERE {\n",
    "  ?item wdt:P2 wd:Q257052.\n",
    "  ?item rdfs:label ?label.\n",
    "  FILTER(LANG(?label) in (\"de\"))\n",
    "}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "r = requests.get(url, params={'query': query}, headers={\"Accept\": \"application/json\"})\n",
    "data = r.json()\n",
    "factgrid_inst_roles_df = pl.json_normalize(data['results']['bindings'])\n",
    "\n",
    "len(factgrid_inst_roles_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Clean Factgrid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract out q id\n",
    "def extract_qid(df, column):\n",
    "    return df.with_columns(pl.col(column).str.strip_chars('https://database.factgrid.de/entity/'))\n",
    "\n",
    "# drop irrelevant columns\n",
    "def drop_type_columns(df):\n",
    "    df = df.drop(\n",
    "        cs.ends_with(\"type\"),\n",
    "        cs.ends_with(\"xml:lang\")\n",
    "        )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factgrid_institution_df = extract_qid(factgrid_institution_df, 'item.value')\n",
    "factgrid_diocese_df = extract_qid(factgrid_diocese_df, 'item.value')\n",
    "factgrid_inst_roles_df = extract_qid(factgrid_inst_roles_df, 'item.value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factgrid_institution_df = drop_type_columns(factgrid_institution_df)\n",
    "factgrid_diocese_df = drop_type_columns(factgrid_diocese_df)\n",
    "factgrid_inst_roles_df = drop_type_columns(factgrid_inst_roles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "factgrid_institution_df.columns = ['fg_institution_id', 'fg_gsn_id']\n",
    "factgrid_diocese_df.columns = [\"fg_diocese_id\", \"dioc_label\", \"dioc_alt\", \"dioc_wiag_id\"]\n",
    "factgrid_inst_roles_df.columns = [\"fg_inst_role_id\", \"inst_role\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the diocese alts by removing BITECA and BETA entries \n",
    "factgrid_diocese_df = factgrid_diocese_df.with_columns(pl.col('dioc_alt').str.replace('^(BITECA|BETA).*', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_fg_entries = factgrid_institution_df.group_by('fg_gsn_id').len().filter(pl.col('len') > 1)\n",
    "if not duplicate_fg_entries.is_empty():\n",
    "    print(duplicate_fg_entries)\n",
    "    raise f\"There are possible institution duplicates on factgrid.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Troubleshooting: possible institution duplicates\n",
    "\n",
    " This can be caused by a simple human error on factgrid.\n",
    "\n",
    " The best solution is to use the factgrid ids printed above and resolve the duplicates.\n",
    "\n",
    "\n",
    "\n",
    " In case you want to ignore the duplicates, uncomment the code below by removing the leading '# ' (keyboard shortcut 'ctrl + /') and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factgrid_institution_df = factgrid_institution_df.filter(pl.col('fg_gsn_id').is_in(duplicate_fg_entries.select('fg_gsn_id').not_()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Join person role data from WIAG with institution and diocese data from Factgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First the WIAG \"Amtsdaten\" for Domherren export is joined with institution data from FactGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_inst_df = role_all_df.join(factgrid_institution_df, how='left', left_on='institution_id', right_on='fg_gsn_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next the diocese data is added.\n",
    "\n",
    "\n",
    "\n",
    " For each entry in the input dataframe, the associated diocese is searched in the factgrid_diocese_df dataframe. The diocese is found by first searching for the WIAG-ID. Only if no entry was found, the search continues with the diocese's name, first in the diocese label and lastly, if the search was unsuccessfull again, in the diocese alt label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with fg dioceses\n",
    "rows = []\n",
    "query = pl.DataFrame() # empty initialisation to enable the call of the clear function below\n",
    "\n",
    "for row in role_inst_df.iter_rows(named = True):\n",
    "    query = query.clear()\n",
    "\n",
    "    if row['diocese_id'] != None:\n",
    "        query = factgrid_diocese_df.filter(pl.col('dioc_wiag_id') == row['diocese_id'])\n",
    "        \n",
    "    if query.is_empty() and row['diocese'] != None:\n",
    "        query = factgrid_diocese_df.filter(pl.col('dioc_label') == row['diocese'])\n",
    "        \n",
    "        if query.is_empty():\n",
    "            query = factgrid_diocese_df.filter(pl.col('dioc_alt') == row['diocese'])\n",
    "\n",
    "    if not query.is_empty():\n",
    "        rows.append({'role_all-id': row['id'], 'fg_diocese_id': query.row(0)[0]})\n",
    "    # #TODO should cases where no result was found be noted/handled?\n",
    "\n",
    "role_inst_dioc_df = role_inst_df.join(pl.DataFrame(rows), how = 'left', left_on = 'id', right_on = 'role_all-id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Check for special cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These lists below allow the code below to identify if the role is missing an institution or if the role doesn't require one at all.\n",
    "\n",
    "\n",
    "\n",
    " * The `unbound_role_groups` list contains the role_groups that are not bound to a place at all.\n",
    "\n",
    "\n",
    "\n",
    " * The `diocese_role_groups` list contains the role_groups that are bound to a diocese but not an institution.\n",
    "\n",
    "   * `diocese_role_group_exception_roles` contains roles that belong to this group but are still bound to an institution.\n",
    "\n",
    "\n",
    "\n",
    " Please add more role_groups or roles to the lists if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbound_role_groups = [\n",
    "    'Kurienamt',\n",
    "    'Papst',\n",
    "    'Kardinal',\n",
    "]\n",
    "diocese_role_groups = [\n",
    "    'Oberstes Leitungsamt Diözese',\n",
    "    'Leitungsamt Diözese',\n",
    "    'Bischöfliches Hilfspersonal',\n",
    "]\n",
    "diocese_role_group_exception_roles = [\n",
    "    None,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all entries that should contain an institution on factgrid but don't have it after the join operation\n",
    "missing_inst_df = role_inst_dioc_df.filter(\n",
    "    pl.col('fg_institution_id').is_null() &\n",
    "    pl.col('role_group').is_in(unbound_role_groups).not_() &\n",
    "    pl.col('role_group').is_in(diocese_role_groups).not_()\n",
    ")\n",
    "print(str(missing_inst_df.height) + \" entries with missing institution id in FG\")\n",
    "\n",
    "# select all entries that should contain a diocese on factgrid but don't have it after the join operation\n",
    "missing_dioc_df = role_inst_dioc_df.filter(\n",
    "    pl.col('fg_diocese_id').is_null() & \n",
    "    pl.col('role_group').is_in(unbound_role_groups).not_() & \n",
    "    pl.col('role_group').is_in(diocese_role_groups) &\n",
    "    pl.col('name').is_in(diocese_role_group_exception_roles).not_()\n",
    ")\n",
    "print(str(missing_dioc_df.height) + \" entries with missing diocese id in FG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check for new roles (roles that so far have not been handled by this notebook)\n",
    "\n",
    " Any roles showing up here need to be added to either the `diocese_role_group_exception_roles` list if they don't need a diocese entry in FactGrid or to the `roles_that_need_a_diocese` list (defined below) if they do need a diocese entry. If you added a name to the `diocese_role_group_exception_roles` list, rerun the cells from the start of step 5 to make sure the change is propagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_that_need_a_diocese = ['Bischof', 'Koadjutor', 'Erzbischof']\n",
    "missing_dioc_df.filter(pl.col('name').is_in(roles_that_need_a_diocese).not_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check entries that have no role group in wiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_inst_df.filter(pl.col('role_group').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check for entries that are missing an id required for the join\n",
    "\n",
    " Please **manually inspect all the entries** that are shown by the code cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Entries that have a missing institution id in WIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_inst_df.filter(pl.col('institution_id').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Entries that have a missing diocese id in WIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dioc_df.filter(pl.col('diocese_id').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create the missing institutions on factgrid here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creates a file with the name institution_creation_\\<date\\>.csv\n",
    "\n",
    "\n",
    "\n",
    " **You need to fill in the empty columns of the file** (except qid) and then use the file on quickstatements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_institution_factgrid_df = missing_inst_df.filter(pl.col('institution_id').is_not_null()).rename({'institution' : 'Lde', 'institution_id' : 'P471'}).unique(subset = pl.col('P471')).with_columns(\n",
    "    qid = None,\n",
    "    Len = None,\n",
    "    Lfr = None,\n",
    "    Les = None,\n",
    "    Dde = None,\n",
    "    Den = None,\n",
    "    P131 = pl.lit('Q153178')\n",
    ").select(['qid', 'Lde', 'Len',\t'Lfr',\t'Les',\t'P471',\t'Dde',\t'Den',\t'P131'])\n",
    "\n",
    "create_institution_factgrid_df.write_csv(file = os.path.join(output_path, f'institution_creation_{today_string}.csv'), separator = ';')\n",
    "\n",
    "create_institution_factgrid_df.sample(n = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Remove all missing (institution and diocese) entries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_missing_entries = pl.concat([missing_inst_df, missing_dioc_df], how = \"diagonal\")\n",
    "\n",
    "dioc_joined_df = role_inst_dioc_df.remove(pl.col(\"id\").is_in(all_missing_entries.get_column(\"id\")))\n",
    "\n",
    "print(\"From originally \" + str(role_inst_dioc_df.height) + \" rows, \" + str(dioc_joined_df.height) + \" rows, that are not missing an institution or diocese, are left.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Add role factgrid id\n",
    "\n",
    " Note: This role does not include the institution information. ie, it adds factgrid ids for roles like 'archbishop' and not 'archbishop of trier'\n",
    "\n",
    "\n",
    "\n",
    " The part of the script below could be used to create quickstatements for career statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Check for roles with multiple entries in FactGrid\n",
    "\n",
    " Should the cell below print anything, these entries need to be **handled manually**, because they contain more than one entry on FactGrid. You can continue with the rest of the notebook even without taking care of these, because these entries will simply be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiag_roles_df.filter(pl.col(\"name\").is_duplicated())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check for missing roles in WIAG role table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_roles_wiag = dioc_joined_df.filter(pl.col(\"name\").is_in(wiag_roles_df.get_column(\"name\")).not_()).unique()\n",
    "print(missing_roles_wiag.height)\n",
    "missing_roles_wiag.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Join role_fg_id attribute from WIAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiag_roles_df = wiag_roles_df.remove(pl.col(\"name\").is_duplicated())\n",
    "\n",
    "joined_df = dioc_joined_df.join(wiag_roles_df.rename({'id' : 'role_id'}), on = \"name\", how = \"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Ignore all Kanonikatsbewerber and Vikariatsbewerber offices\n",
    "\n",
    " TODO: add reason here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.remove(pl.col('name').is_in(['Vikariatsbewerber', 'Kanonikatsbewerber'])) # TODO why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Entries with missing factgrid entries for the roles in wiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_roles_df = joined_df.filter(pl.col('role_fg_id').is_null())\n",
    "print(str(missing_roles_df.height) + \" entries are missing a role in FactGrid.\\n\")\n",
    "\n",
    "print(\"Roles that are not yet in FactGrid:\")\n",
    "missing_roles = missing_roles_df.select(pl.col('name'), pl.col('role_id'), pl.col('role_group_fq_id')).unique().drop_nulls() # TODO report null values, instead of just dropping them\n",
    "missing_roles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Create a csv file to be manually filled and later read to generate quickstatements\n",
    "\n",
    "\n",
    "\n",
    " Make changes to this file and then upload it to quickstatements (don't forget to remove the item_id column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_missing_roles_df = missing_roles.with_columns(\n",
    "    qid = None,\n",
    "    Len = None,\n",
    "    Lfr = None,\n",
    "    Les = None,\n",
    "    Dde = None,\n",
    "    Den = None,\n",
    "    P2 = pl.lit(\"Q37073\"),\n",
    "    P131 = pl.lit(\"Q153178\")\n",
    ").rename({\n",
    "    \"name\" : \"Lde\",\n",
    "    \"role_id\" : \"item_id\",\n",
    "    \"role_group_fq_id\" : \"P3\"}\n",
    ").select(\n",
    "    [\"qid\",\t\"Lde\",\t\"Len\",\t\"Lfr\",\t\"Dde\",\t\"Den\",\t\"P2\",\t\"P131\",\t\"item_id\",\t\"P3\"]\n",
    ")\n",
    "\n",
    "create_missing_roles_df.write_csv(os.path.join(output_path, f\"create-missing-roles-{today_string}.csv\"), separator = ';')\n",
    "create_missing_roles_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Remove all missing (role) entries now **\n",
    "\n",
    " The code below removes all the entries that failed the join with the wiag role join above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_roles_in_fg_df = joined_df.remove(pl.col('role_fg_id').is_null())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check people with missing factgrid entries or missing factgrid ids in wiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_people_list = joined_df.filter(pl.col('FactGrid').is_null()).unique('person_id')\n",
    "print(missing_people_list.height)\n",
    "missing_people_list.sample(n = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Generate the quickstatements for creating the persons\n",
    "\n",
    "\n",
    "\n",
    " Go back to step 5 (Csv2FactGrid-create) to create the missing persons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Remove all missing (person) entries now **\n",
    "\n",
    " The code below removes all the entries for persons that don't exist on factgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(joined_df))\n",
    "joined_df = joined_df.filter(pl.col('FactGrid').is_not_null())\n",
    "print(len(joined_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Add factgrid ids for roles (with institution)\n",
    "\n",
    " Note: this role has information of the institution as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in addition to the parameters, uses the dataframe factgrid_inst_roles_df directly\n",
    "\n",
    "def find_fg_inst_role(name, inst, dioc):\n",
    "    search_result = pl.DataFrame()\n",
    "    if inst == None:\n",
    "        if dioc != None: # TODO handle cases where inst and dioc are None? - should only be true for [35, 48, 49] Kardinal, Papst, Kurienamt (except maybe special role_groups)\n",
    "            if name not in [\"Archidiakon\", \"Koadjutor\"]:\n",
    "                dioc = dioc.lstrip('Bistum').lstrip('Erzbistum').lstrip('Patriarchat').lstrip()\n",
    "            if name == \"Fürstbischof\" and dioc in [\"Passau\", \"Straßburg\"]:\n",
    "                name = \"Bischof\"    \n",
    "            search_result = factgrid_inst_roles_df.filter(pl.col('inst_role').str.contains(f\"^{name}.*{dioc}\"))\n",
    "            if name == \"Erzbischof\" and dioc == \"Salzburg\":\n",
    "                # will be merged in later # TODO what does this mean and why?\n",
    "                search_result = factgrid_inst_roles_df.filter(pl.col('fg_inst_role_id') == 'Q172567')\n",
    "    else:\n",
    "        name = name.replace('Domkanoniker', 'Domherr')\n",
    "        search_result = factgrid_inst_roles_df.filter(pl.col('inst_role') == f\"{name} {inst}\")\n",
    "    \n",
    "    return search_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [] # joined to the main df as fg_inst_role_id (used in the last part) - in other words, these are the institution roles that are assigned on FactGrid\n",
    "not_found = [] # used for creating institution roles (e.g. bishop of ...) in the next cell\n",
    "dupl = {} # these entries are ignored, because they need to be fixed manually\n",
    "\n",
    "i = 0\n",
    "for (id, name, inst, inst_id, dioc) in joined_df.select('id', 'name', 'institution', 'institution_id', 'diocese').iter_rows():\n",
    "    # Kardinal receives insitution role Q254893 manually -- probably simply handling a simple special case first\n",
    "    if name == \"Kardinal\":\n",
    "        data_dict.append((id,\"Q254893\"))\n",
    "        continue\n",
    "    \n",
    "    search_result = find_fg_inst_role(name, inst, dioc)\n",
    "\n",
    "    if search_result.is_empty() or len(search_result) == 0:\n",
    "        # TODO entries without institution entry in WIAG are simply ignored - makes sense if dioc is set?? (diocese level roles)\n",
    "        not_found.append((name, inst, inst_id))\n",
    "    elif len(search_result) == 1:\n",
    "        data_dict.append((id, search_result['fg_inst_role_id'][0]))\n",
    "    elif len(search_result) >= 2:\n",
    "        dupl[i] = (name, inst, dioc, search_result)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "print(\"Roles found:\", len(data_dict), \"duplicates:\", len(dupl), \"not found:\", len(not_found))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Create entries for missing inst roles on factgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_df = pl.DataFrame(not_found, orient = 'row', schema = ['role', 'institution', 'institution_id'])\n",
    "not_found_df = not_found_df.drop_nulls() # remove entries for diocese level roles \n",
    "\n",
    "# not_found contains an entry per row where a combination was not found - here we want just one row per unique combination\n",
    "# these combinations could be found much more efficiently, but as it's a byproduct of finding the fg_inst_role_id for all the other rows, this is fine\n",
    "not_found_df = not_found_df.unique()\n",
    "# since the institution names are quite specific, it's not realistic that two roles with the same label but different institution_id could exist\n",
    "\n",
    "# add role details\n",
    "not_found_df = not_found_df.join(\n",
    "    wiag_roles_df.select('id', 'name', 'role_fg_id'), how='left', left_on='role', right_on='name'\n",
    ")\n",
    "# add instution details\n",
    "not_found_df = not_found_df.join(factgrid_institution_df, how='left', left_on='institution_id', right_on='fg_gsn_id')\n",
    "\n",
    "# add other columns\n",
    "not_found_df = not_found_df.with_columns(\n",
    "    qid = None,\n",
    "    Lde = pl.col('role') + ' ' + pl.col('institution'),\n",
    "    Len = None,\n",
    "    Lfr = None,\n",
    "    Les = None,\n",
    "    Dde = None,\n",
    "    Den = None,\n",
    "    P2 = pl.lit('Q257052'),\n",
    "    P131 = pl.lit('Q153178'),\n",
    "    P3 = pl.col('role_fg_id'),\n",
    "    P267 = pl.col('fg_institution_id'),\n",
    "    # id is the number of the role in the role table in wiag -- institution_id is the klosterdatenbank id of the institution\n",
    "    P1100 = pl.when(pl.col('id').is_null()).then(pl.lit(None)).otherwise('off' + pl.col('id').cast(str) + '_gsn' + pl.col('institution_id').cast(str))\n",
    ").select(['qid', 'Lde', 'Len', 'Dde', 'Den', 'P2', 'P131', 'P3', 'P267', 'P1100']) # selecting only relevant columns\n",
    "\n",
    "# export to csv file\n",
    "not_found_df.write_csv(os.path.join(output_path, f\"create-missing-inst-roles-{today_string}.csv\"), separator=';')\n",
    "print(f'{not_found_df.height} rows were written. A sample of them:')\n",
    "not_found_df.sample(n = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Ignore all missing (inst role) entries now **\n",
    "\n",
    " The code below ignores entries that are generated above and does a join without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_joined_df = joined_df.join(pl.DataFrame(data_dict, schema = ['id', 'fg_inst_role_id'], orient = 'row'), on = 'id')\n",
    "print(len(final_joined_df))\n",
    "final_joined_df.sample(n = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Parse begin and end date from the wiag data\n",
    "\n",
    "\n",
    "\n",
    " The following code parses the date information present in the date_begin or date_end string and converts it to the correct property in factgrid and it's corresponding value.\n",
    "\n",
    " There are also testcases which are run in case you want to modify it.\n",
    "\n",
    "\n",
    "\n",
    " Here is an overview of relevant FactGrid properties: [link](https://database.factgrid.de/query/embed.html#SELECT%20%3FPropertyLabel%20%3FProperty%20%3FPropertyDescription%20%3Freciprocal%20%3FreciprocalLabel%20%3Fexample%20%3Fuseful_statements%20%3Fwd%20WHERE%20%7B%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22.%20%7D%0A%20%20%3FProperty%20wdt%3AP8%20wd%3AQ77483.%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP364%20%3Fexample.%20%7D%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP86%20%3Freciprocal.%20%7D%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP343%20%3Fwd.%20%7D%0A%20%20OPTIONAL%20%7B%20%3FProperty%20wdt%3AP310%20%3Fuseful_statements.%20%7D%0A%7D%0AORDER%20BY%20%3FPropertyLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an enum to more clearly define what type of date is being passed \n",
    "from enum import Enum\n",
    "class DateType(Enum):\n",
    "    ONLY_DATE = 0\n",
    "    BEGIN_DATE = 1\n",
    "    END_DATE = 2\n",
    "\n",
    "# defining some constants for better readability of the code:\n",
    "# self defined:\n",
    "JULIAN_ENDING = '/J'\n",
    "JHS_GROUP = r'(Jhs\\.|Jahrhunderts?)'\n",
    "JH_GROUP = r'(Jh\\.|Jahrhundert)'\n",
    "EIGTH_OF_A_CENTURY = 13\n",
    "QUARTER_OF_A_CENTURY = 25\n",
    "TENTH_OF_A_CENTURY = 10\n",
    "\n",
    "ANTE_GROUP = \"bis|vor|spätestens\"\n",
    "POST_GROUP = \"nach|frühestens|ab\"\n",
    "CIRCA_GROUP = r\"etwa|ca\\.|um\"\n",
    "# pre-compiling the most complex pattern to increase efficiency\n",
    "MOST_COMPLEX_PATTERN = re.compile(r'(wohl )?((kurz )?(' + ANTE_GROUP + '|' + POST_GROUP + r') )?((' + CIRCA_GROUP +r') )?(\\d{3,4})(\\?)?')\n",
    "\n",
    "# FactGrid properties:\n",
    "    # simple date properties:\n",
    "DATE = 'P106' \n",
    "BEGIN_DATE = 'P49'\n",
    "END_DATE = 'P50'\n",
    "    # when there is uncertainty / when all we know is the latest/earliest possible date:\n",
    "DATE_AFTER = 'P41' # the earliest possible date for something\n",
    "DATE_BEFORE = 'P43' # the latest possible date for something\n",
    "END_TERMINUS_ANTE_QUEM = 'P1123' # latest possible date of the end of a period\n",
    "BEGIN_TERMINUS_ANTE_QUEM  = 'P1124' # latest possible date of the begin of a period\n",
    "END_TERMINUS_POST_QUEM = 'P1125' # earliest possible date of the end of a period\n",
    "BEGIN_TERMINUS_POST_QUEM = 'P1126' # earliest possible date of the beginning of a period\n",
    "\n",
    "NOTE = 'P73' # Field for free notes\n",
    "PRECISION_DATE = 'P467' # FactGrid qualifier for the specific determination of the exactness of a date\n",
    "PRECISION_BEGIN_DATE = 'P785'   # qualifier to specify a begin date\n",
    "PRECISION_END_DATE = 'P786'\n",
    "STRING_PRECISION_BEGIN_DATE = 'P787' # qualifier to specify a begin date; string alternate to P785\n",
    "STRING_PRECISION_END_DATE = 'P788'\n",
    "\n",
    "def format_datetime(entry: datetime, resolution):\n",
    "    ret_val =  f\"+{entry.isoformat()}Z/{resolution}\"\n",
    "\n",
    "    if entry.year < 1582:\n",
    "        ret_val +=  JULIAN_ENDING\n",
    "    \n",
    "    if resolution <= 9:\n",
    "        ret_val = ret_val.replace(f\"{entry.year}-01-01\", f\"{entry.year}-00-00\", 1)\n",
    "\n",
    "    return ret_val\n",
    "\n",
    "# only_date=True means there is only one date, not a 'begin date' and an 'end date'\n",
    "def date_parsing(date_string: str, date_type: DateType):\n",
    "    qualifier = None\n",
    "    entry = None\n",
    "    resolution = 7\n",
    "\n",
    "    ante_property = (match := re.search(ANTE_GROUP, date_string))\n",
    "    post_property = (match := re.search(POST_GROUP, date_string))\n",
    "    assert(not ante_property or not post_property)\n",
    "    \n",
    "    match date_type:\n",
    "        case DateType.ONLY_DATE:\n",
    "            string_precision_qualifier_clause = NOTE\n",
    "            exact_precision_qualifier = PRECISION_DATE\n",
    "            if ante_property:\n",
    "                return_property = DATE_BEFORE\n",
    "            elif post_property:\n",
    "                return_property = DATE_AFTER\n",
    "            else:\n",
    "                return_property = DATE\n",
    "        case DateType.BEGIN_DATE:\n",
    "            string_precision_qualifier_clause = STRING_PRECISION_BEGIN_DATE\n",
    "            exact_precision_qualifier = PRECISION_BEGIN_DATE\n",
    "            if ante_property:\n",
    "                return_property = BEGIN_TERMINUS_ANTE_QUEM\n",
    "            elif post_property:\n",
    "                return_property = BEGIN_TERMINUS_POST_QUEM\n",
    "            else:\n",
    "                return_property = BEGIN_DATE\n",
    "        case DateType.END_DATE:\n",
    "            string_precision_qualifier_clause = STRING_PRECISION_END_DATE\n",
    "            exact_precision_qualifier = PRECISION_END_DATE\n",
    "            if ante_property:\n",
    "                return_property = END_TERMINUS_ANTE_QUEM\n",
    "            elif post_property:\n",
    "                return_property = END_TERMINUS_POST_QUEM\n",
    "            else:\n",
    "                return_property = END_DATE    \n",
    "        case _:\n",
    "            assert False, \"Unexpected DateType!\"\n",
    "        \n",
    "    string_precision_qualifier_clause += f'\\t\"{date_string}\"'\n",
    "\n",
    "    if date_string == '?':\n",
    "        return tuple()\n",
    "            \n",
    "    if matches := re.match(r'(\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        centuries = int(matches.group(1))\n",
    "        entry = datetime(100 * (centuries), 1, 1)\n",
    "    \n",
    "    elif matches := re.match(r'(\\d)\\. Hälfte (des )?(\\d{1,2})\\. ' + JHS_GROUP, date_string):\n",
    "        half = int(matches.group(1)) - 1\n",
    "        centuries = int(matches.group(3)) - 1\n",
    "        year   = centuries * 100 + (half * 50) + QUARTER_OF_A_CENTURY\n",
    "        entry = datetime(year, 1, 1)\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "    \n",
    "    elif matches := re.match(r'(\\w+) Viertel des (\\d{1,2})\\. ' + JHS_GROUP, date_string):\n",
    "        number_map = {\n",
    "            \"erstes\":  0,\n",
    "            \"zweites\": 1,\n",
    "            \"drittes\": 2,\n",
    "            \"viertes\": 3,\n",
    "        }\n",
    "        quarter = matches.group(1)\n",
    "        centuries = int(matches.group(2))\n",
    "        year = (centuries - 1) * 100 + (number_map[quarter] * 25) + EIGTH_OF_A_CENTURY\n",
    "        entry = datetime(year, 1, 1)\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'frühes (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        centuries = int(matches.group(1)) - 1\n",
    "        year = centuries * 100 + TENTH_OF_A_CENTURY\n",
    "        entry = datetime(year, 1, 1)\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'spätes (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        centuries = int(matches.group(1))\n",
    "        year = centuries * 100 - TENTH_OF_A_CENTURY\n",
    "        entry = datetime(year, 1, 1)\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'(Anfang|Mitte|Ende) (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        number_map = {\n",
    "            \"Anfang\":  0,\n",
    "            \"Mitte\": 1,\n",
    "            \"Ende\": 2,\n",
    "        }\n",
    "        third = number_map[matches.group(1)]\n",
    "        centuries = int(matches.group(2)) - 1\n",
    "        year = centuries * 100 + (third * 33) + 17\n",
    "        entry = datetime(year, 1, 1)\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'(\\d{3,4})er Jahre', date_string):\n",
    "        entry = datetime(int(matches.group(1)), 1, 1)\n",
    "        resolution = 8\n",
    "    \n",
    "    elif matches := re.match(r'Wende zum (\\d{1,2})\\. ' + JH_GROUP, date_string):\n",
    "        centuries = int(matches.group(1)) - 1\n",
    "        entry = datetime(centuries * 100 + 10, 1, 1)\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'Anfang der (\\d{3,4})er Jahre', date_string):\n",
    "        entry = datetime(int(matches.group(1)), 1, 1)\n",
    "        resolution = 8\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "\n",
    "    elif matches := re.match(r'\\((\\d{3,4})\\s?\\?\\) (\\d{3,4})', date_string):\n",
    "        entry = datetime(int(matches.group(2)), 1, 1) # ignoring the year in parantheses\n",
    "        resolution = 9\n",
    "        qualifier = string_precision_qualifier_clause\n",
    "    \n",
    "    elif matches := re.match(r'(\\d{3,4})/(\\d{3,4})', date_string):\n",
    "        year1 = int(matches.group(1))\n",
    "        year2 = int(matches.group(2))\n",
    "\n",
    "        if year2 - year1 == 1:\n",
    "            # check for consecutive years\n",
    "            qualifier = exact_precision_qualifier + \"\\tQ912616\"\n",
    "        entry = datetime(year1, 1, 1)\n",
    "        resolution = 9\n",
    "\n",
    "    # this pattern is pre-compiled above, because it's rather complex and it's much more efficient to compile it just once, instead of on every function call\n",
    "    elif matches := MOST_COMPLEX_PATTERN.match(date_string):\n",
    "        if matches.group(1): # if 'wohl' was found\n",
    "            qualifier = exact_precision_qualifier + '\\tQ23356'\n",
    "        if matches.group(5): # if 'etwa' , 'ca.' or 'um' were found\n",
    "            if qualifier == None:\n",
    "                qualifier = exact_precision_qualifier + \"\\tQ10\"\n",
    "            else:\n",
    "                qualifier += '\\t' + exact_precision_qualifier + \"\\tQ10\"\n",
    "\n",
    "        if matches.group(3) or matches.group(8): # if 'kurz' or a question mark at the end were found\n",
    "            # TODO is it correct, that on ? the other matches ('ca.' etc.) are ignored, because it's not exact enough?\n",
    "            qualifier = string_precision_qualifier_clause\n",
    "        \n",
    "        entry = datetime(int(matches.group(7)), 1, 1)\n",
    "        resolution = 9\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"Couldn't parse date '{date_string}'\")\n",
    "        \n",
    "    if qualifier:\n",
    "        return (return_property, format_datetime(entry, resolution), qualifier)\n",
    "    else:\n",
    "        return (return_property, format_datetime(entry, resolution))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code below is **just for testing**. So long as you don't change anything in the code cell above, you can just skip over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because there are so many special cases, testing is a must to more clearly show what is expected for each case and make sure no incorrect changes are made.\n",
    "# TODO why resolution of 7, 8 or 9?\n",
    "\n",
    "# still to be handled:\n",
    "    # \"Ende 11. Jahrhundert/1. Viertel 12. Jahrhundert\": \"\", TODO what date?\n",
    "    # \"(996)\" #TODO mistake or what does this mean?\n",
    "    # \"12. oder 13. Jahrhundert\"\n",
    "    # \"(vor 1254) 1256\"\n",
    "\n",
    "begin_date_tests = {\n",
    "    \"1205\": (BEGIN_DATE, \"+1205-00-00T00:00:00Z/9/J\"),\n",
    "    \"1205?\": (BEGIN_DATE, \"+1205-00-00T00:00:00Z/9/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"1205?\"'),\n",
    "    \"12. Jahrhundert\": (BEGIN_DATE, \"+1200-00-00T00:00:00Z/7/J\"),\n",
    "    \"1. Hälfte des 12. Jhs.\": (BEGIN_DATE, \"+1125-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"1. Hälfte des 12. Jhs.\"'),\n",
    "    \"1. Hälfte des 12. Jahrhunderts\": (BEGIN_DATE, \"+1125-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"1. Hälfte des 12. Jahrhunderts\"'),\n",
    "    \"2. Hälfte des 12. Jhs.\": (BEGIN_DATE, \"+1175-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"2. Hälfte des 12. Jhs.\"'),\n",
    "    \"erstes Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1113-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"erstes Viertel des 12. Jhs.\"'),\n",
    "    \"zweites Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1138-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"zweites Viertel des 12. Jhs.\"'),\n",
    "    \"drittes Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1163-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"drittes Viertel des 12. Jhs.\"'),\n",
    "    \"viertes Viertel des 12. Jhs.\": (BEGIN_DATE, \"+1188-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"viertes Viertel des 12. Jhs.\"'),\n",
    "    \"frühes 12. Jh.\": (BEGIN_DATE, \"+1110-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"frühes 12. Jh.\"'),\n",
    "    \"spätes 12. Jh.\": (BEGIN_DATE, \"+1190-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"spätes 12. Jh.\"'),\n",
    "    \"Anfang 12. Jh.\": (BEGIN_DATE, \"+1117-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Anfang 12. Jh.\"'),\n",
    "    \"Anfang 15. Jahrhundert\": (BEGIN_DATE, \"+1417-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Anfang 15. Jahrhundert\"'),\n",
    "    \"Mitte 12. Jh.\": (BEGIN_DATE, \"+1150-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Mitte 12. Jh.\"'),\n",
    "    \"Mitte 14. Jahrhundert?\": (BEGIN_DATE, \"+1350-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Mitte 14. Jahrhundert?\"'),\n",
    "    \"Ende 12. Jh.\": (BEGIN_DATE, \"+1183-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Ende 12. Jh.\"'),\n",
    "    \"Ende 12. Jahrhundert\": (BEGIN_DATE, \"+1183-00-00T00:00:00Z/7/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"Ende 12. Jahrhundert\"'),\n",
    "    \"bis etwa 1147\": (BEGIN_TERMINUS_ANTE_QUEM, '+1147-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\tQ10'),\n",
    "    \"etwa 1147\": (BEGIN_DATE, '+1147-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\tQ10'),\n",
    "    \"ca. 1050\": (BEGIN_DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_BEGIN_DATE + '\\tQ10'),\n",
    "    \"um 1050\": (BEGIN_DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_BEGIN_DATE + '\\tQ10'),\n",
    "    \"1230er Jahre\": (BEGIN_DATE, \"+1230-00-00T00:00:00Z/8/J\"), # TODO: shouldn't it be BEGIN_TERMINUS_POST_QUEM?\n",
    "    \"Wende zum 12. Jh.\": (BEGIN_DATE, '+1110-00-00T00:00:00Z/7/J', STRING_PRECISION_BEGIN_DATE + '\\t\"Wende zum 12. Jh.\"'), # TODO: why not 1100?\n",
    "    \"Anfang der 1480er Jahre\": (BEGIN_DATE, '+1480-00-00T00:00:00Z/8/J', STRING_PRECISION_BEGIN_DATE + '\\t\"Anfang der 1480er Jahre\"'),\n",
    "    \"1164/1165\": (BEGIN_DATE, '+1164-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\tQ912616'),\n",
    "    \"1164/1177\": (BEGIN_DATE, '+1164-00-00T00:00:00Z/9/J'),\n",
    "    \"(1014?) 1015\": (BEGIN_DATE,\"+1015-00-00T00:00:00Z/9/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"(1014?) 1015\"'),\n",
    "    \"ab 1534\": (BEGIN_TERMINUS_POST_QUEM, '+1534-00-00T00:00:00Z/9/J'),\n",
    "    \"nach 1230\": (BEGIN_TERMINUS_POST_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"kurz nach 1200\": (BEGIN_TERMINUS_POST_QUEM, '+1200-00-00T00:00:00Z/9/J', STRING_PRECISION_BEGIN_DATE + '\\t\"kurz nach 1200\"'),\n",
    "    #\"kurz nach 1200\": (BEGIN_TERMINUS_POST_QUEM, '+1200-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\tQ266009'), # TODO correct?\n",
    "    \"frühestens 1342\": (BEGIN_TERMINUS_POST_QUEM, '+1342-00-00T00:00:00Z/9/J'),\n",
    "    \"vor 1230\": (BEGIN_TERMINUS_ANTE_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"wohl vor 1249\": (BEGIN_TERMINUS_ANTE_QUEM, '+1249-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\tQ23356'),\n",
    "    \"kurz vor 1200\": (BEGIN_TERMINUS_ANTE_QUEM, '+1200-00-00T00:00:00Z/9/J', STRING_PRECISION_BEGIN_DATE + '\\t\"kurz vor 1200\"'),\n",
    "    #\"kurz vor 1200\": (BEGIN_TERMINUS_ANTE_QUEM, '+1200-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\tQ255211'), # TODO correct?\n",
    "    \"wohl etwa 1249\": (BEGIN_DATE, '+1249-00-00T00:00:00Z/9/J', PRECISION_BEGIN_DATE + '\\tQ23356\\tP785\\tQ10'), # TODO correct or should they not be combined?\n",
    "    \"spätestens 1277\": (BEGIN_TERMINUS_ANTE_QUEM, '+1277-00-00T00:00:00Z/9/J'),\n",
    "    #\"zwischen 1087 und 1093\": (BEGIN_TERMINUS_POST_QUEM,\"+1087-00-00T00:00:00Z/9/J\", STRING_PRECISION_BEGIN_DATE + '\\t\"zwischen 1087 und 1093\"'), # TODO correct? or maybe BEGIN_TERMINUS_ANTE_QUEM 1093?\n",
    "}\n",
    "\n",
    "for key, value in begin_date_tests.items():\n",
    "    retval = date_parsing(key, DateType.BEGIN_DATE)\n",
    "    assert retval == value, f\"{key}: Returned {retval} instead of {value}\"\n",
    "\n",
    "end_date_tests = {\n",
    "    \"1205?\": (END_DATE, \"+1205-00-00T00:00:00Z/9/J\", STRING_PRECISION_END_DATE + '\\t\"1205?\"'),\n",
    "    \"12. Jahrhundert\": (END_DATE, \"+1200-00-00T00:00:00Z/7/J\"),\n",
    "    \"drittes Viertel des 12. Jhs.\": (END_DATE, \"+1163-00-00T00:00:00Z/7/J\", STRING_PRECISION_END_DATE + '\\t\"drittes Viertel des 12. Jhs.\"'),\n",
    "    \"bis etwa 1147\": (END_TERMINUS_ANTE_QUEM, '+1147-00-00T00:00:00Z/9/J', PRECISION_END_DATE + '\\tQ10'),\n",
    "    \"um 1050\": (END_DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_END_DATE + '\\tQ10'),\n",
    "    \"Anfang der 1480er Jahre\": (END_DATE, '+1480-00-00T00:00:00Z/8/J', STRING_PRECISION_END_DATE + '\\t\"Anfang der 1480er Jahre\"'),\n",
    "    \"1164/1165\": (END_DATE, '+1164-00-00T00:00:00Z/9/J', PRECISION_END_DATE + '\\tQ912616'),\n",
    "    \"1164/1177\": (END_DATE, '+1164-00-00T00:00:00Z/9/J'),\n",
    "    \"(1014?) 1015\": (END_DATE,\"+1015-00-00T00:00:00Z/9/J\", STRING_PRECISION_END_DATE + '\\t\"(1014?) 1015\"'),\n",
    "    \"ab 1534\": (END_TERMINUS_POST_QUEM, '+1534-00-00T00:00:00Z/9/J'),\n",
    "    \"nach 1230\": (END_TERMINUS_POST_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"frühestens 1342\": (END_TERMINUS_POST_QUEM, '+1342-00-00T00:00:00Z/9/J'),\n",
    "    \"vor 1230\": (END_TERMINUS_ANTE_QUEM, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"wohl vor 1249\": (END_TERMINUS_ANTE_QUEM, '+1249-00-00T00:00:00Z/9/J', PRECISION_END_DATE + '\\tQ23356'),\n",
    "    #\"zwischen 1087 und 1093\": (BEGIN_TERMINUS_POST_QUEM,\"+1087-00-00T00:00:00Z/9/J\", STRING_PRECISION_END_DATE + '\\t\"zwischen 1087 und 1093\"'), # TODO correct? or maybe BEGIN_TERMINUS_ANTE_QUEM 1093?\n",
    "}\n",
    "\n",
    "for key, value in end_date_tests.items():\n",
    "    retval = date_parsing(key, DateType.END_DATE)\n",
    "    assert retval == value, f\"{key}: Returned {retval} instead of {value}\"\n",
    "\n",
    "only_date_tests = {\n",
    "    \"1205?\": (DATE, \"+1205-00-00T00:00:00Z/9/J\", NOTE + '\\t\"1205?\"'),\n",
    "    \"12. Jahrhundert\": (DATE, \"+1200-00-00T00:00:00Z/7/J\"),\n",
    "    \"drittes Viertel des 12. Jhs.\": (DATE, \"+1163-00-00T00:00:00Z/7/J\", NOTE + '\\t\"drittes Viertel des 12. Jhs.\"'),\n",
    "    \"bis etwa 1147\": (DATE_BEFORE, '+1147-00-00T00:00:00Z/9/J', PRECISION_DATE + '\\tQ10'),\n",
    "    \"um 1050\": (DATE, \"+1050-00-00T00:00:00Z/9/J\", PRECISION_DATE + '\\tQ10'),\n",
    "    \"Anfang der 1480er Jahre\": (DATE, '+1480-00-00T00:00:00Z/8/J', NOTE + '\\t\"Anfang der 1480er Jahre\"'),\n",
    "    \"1164/1165\": (DATE, '+1164-00-00T00:00:00Z/9/J', PRECISION_DATE + '\\tQ912616'),\n",
    "    \"1164/1177\": (DATE, '+1164-00-00T00:00:00Z/9/J'),\n",
    "    \"(1014?) 1015\": (DATE,\"+1015-00-00T00:00:00Z/9/J\", NOTE + '\\t\"(1014?) 1015\"'),\n",
    "    \"ab 1534\": (DATE_AFTER, '+1534-00-00T00:00:00Z/9/J'),\n",
    "    \"nach 1230\": (DATE_AFTER, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"frühestens 1342\": (DATE_AFTER, '+1342-00-00T00:00:00Z/9/J'),\n",
    "    \"vor 1230\": (DATE_BEFORE, '+1230-00-00T00:00:00Z/9/J'),\n",
    "    \"wohl vor 1249\": (DATE_BEFORE, '+1249-00-00T00:00:00Z/9/J', PRECISION_DATE + '\\tQ23356'),\n",
    "    #\"zwischen 1087 und 1093\": (DATE,\"+1087-00-00T00:00:00Z/9/J\", NOTE + '\\t\"zwischen 1087 und 1093\"'), # TODO maybe 1090 instead?\n",
    "}\n",
    "\n",
    "for key, value in only_date_tests.items():\n",
    "    retval = date_parsing(key, DateType.ONLY_DATE)\n",
    "    assert retval == value, f\"{key}: Returned {retval} instead of {value}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Reconcile office data with factgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_joined_df.head() # TODO which entries are being updated? -> how to check on FG what is already there? https://database.factgrid.de/wiki/Special:EntityData/Q515.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Generate quickstatements for offices\n",
    "\n",
    " The code below creates the office entries to be uploaded on factgrid.\n",
    "\n",
    " If the date parsing fails, the corresponding date string is printed out and along with the entry.\n",
    "\n",
    "\n",
    "\n",
    " When the parsing fails, sometimes the date parsing defined above needs to be extended to handle cases that haven't been handled until now and sometimes entries in WIAG need to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(output_path, f'quickstatements_{today_string}.qs')\n",
    "\n",
    "with open(filepath, 'w') as file:\n",
    "    for row in final_joined_df.iter_rows(named = True):\n",
    "        try:\n",
    "            date_clauses = ()\n",
    "\n",
    "            if row['date_begin'] != None:\n",
    "                if row['date_end'] != None:\n",
    "                    date_clauses = (*date_parsing(row['date_begin'], DateType.BEGIN_DATE), *date_parsing(row['date_end'], DateType.END_DATE))\n",
    "                else:\n",
    "                    date_clauses = date_parsing(row['date_begin'], DateType.ONLY_DATE)\n",
    "            else:\n",
    "                if row['date_end'] != None:\n",
    "                    date_clauses = date_parsing(row['date_end'], DateType.ONLY_DATE)\n",
    "                    \n",
    "            file.write('\\t'.join([\n",
    "                row['FactGrid'], \n",
    "                'P165', \n",
    "                row['fg_inst_role_id'],\n",
    "                'S601', \n",
    "                '\"' + row['person_id'] + '\"',\n",
    "                *date_clauses,\n",
    "            ]) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc())\n",
    "            print(row)\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
